{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "CNN is basically a model known to be Convolutional Neural Network and in the recent time it has gained a lot of popularity because of itâ€™s usefullness. CNN uses multilayer perceptrons to do computational works. CNNs use relatively little pre-processing compared to other image classification algorithms. This means the network learns through filters that in traditional algorithms were hand-engineered. So, for image processing task CNNs are the best-suited option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#include all necessary python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "Keras is a high-level neural network API, written in Python which runs on top of either Tensorflow or Theano.\n",
    "Tensorflow was developed by the Google Brain team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include all necessary Keras libraries\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.random.seed(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of X_train is (60000, 28, 28). Each image has 28 x 28 resolution. The shape of X_test is (10000, 28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train original shape (60000, 28, 28)\n",
      "y_train original shape (60000,)\n",
      "X_test original shape (10000, 28, 28)\n",
      "y_test original shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)\n",
    "print(\"X_test original shape\", X_test.shape)\n",
    "print(\"y_test original shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Class 5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualisation of one particular image\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title('Class '+ str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape that a CNN accepts should be in a specific format. If you are using Tensorflow, the format should be (batch, height, width, channels). If you are using Theano, the format should be (batch, channels, height, width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In CNN, we can normalize data before hands such that large terms of the calculations can be reduced to smaller terms.\n",
    "#Like, we can normalize the x_train and x_test data by dividing it with 255.\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "#As all the images are in grayscale, the number of channels is 1.\n",
    "#If it was a color image, then the number of channels would be 3 (R, G, B).\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train/=255\n",
    "X_test/=255\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding\n",
    "In one-hot encoding an integer is converted to an array which contains only one â€˜1â€™ and the rest elements are â€˜0â€™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_classes = 10\n",
    "# convert class vectors to binary class matrices.One-Hot Encoding\n",
    "Y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "\n",
    "y_train[0], Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three steps to Convolution\n",
    "# 1. Convolution\n",
    "# 2. Activation\n",
    "# 3. Polling\n",
    "# Repeat Steps 1,2,3 for adding more hidden layers\n",
    "\n",
    "# 4. After that make a fully connected network\n",
    "# This fully connected network gives ability to the CNN\n",
    "# to classify the samples\n",
    "\n",
    "\n",
    "#There are two ways to build Keras models: sequential and functional.\n",
    "\n",
    "#The sequential API allows you to create models layer-by-layer for most problems.\n",
    "#It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs.\n",
    "\n",
    "model = Sequential() \n",
    "\n",
    "#32 is number of filters and (3, 3) is the size of the filter.\n",
    "#We also need to specify the shape of the input which is (28, 28, 1), but we have to specify it only once.\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n",
    "model.add(Activation('relu'))\n",
    "BatchNormalization(axis=-1)\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "BatchNormalization(axis=-1)\n",
    "model.add(Conv2D(64,(3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "BatchNormalization(axis=-1)\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "# Fully connected layer\n",
    "\n",
    "BatchNormalization()\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "BatchNormalization()\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "\n",
    "# model.add(Convolution2D(10,3,3, border_mode='same'))\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "The second layer is the Activation layer. We have used ReLU (rectified linear unit) as our activation function. ReLU function is f(x) = max(0, x), where x is the input. It sets all negative values in the matrix â€˜xâ€™ to 0 and keeps all the other values constant. It is the most used activation function since it reduces training time\n",
    "\n",
    "## MAXPOOLING\n",
    "The third layer is the MaxPooling layer. MaxPooling layer is used to down-sample the input to enable the model to make assumptions about the features so as to reduce over-fitting. It also reduces the number of parameters to learn, reducing the training time.\n",
    "\n",
    "### BatchNormalization\n",
    " It normalizes the matrix after it is been through a convolution layer so that the scale of each dimension remains the same. It reduces the training time significantly.\n",
    "\n",
    "#### Flattening\n",
    "After creating all the convolutional layers, we need to flatten them, so that they can act as an input to the Dense layers.\n",
    "\n",
    "#### Dense Layers\n",
    "Dense layers are kerasâ€™s alias for Fully connected layers. These layers give the ability to classify the features learned by the CNN.\n",
    "\n",
    "#### Dropout\n",
    "Dropout is the method used to reduce overfitting. It forces the model to learn multiple independent representations of the same data by randomly disabling neurons in the learning phase. In our model, dropout will randomnly disable 20% of the neurons.\n",
    "\n",
    "The second last layer is the Dense layer with 10 neurons. The neurons in this layer should be equal to the number of classes we want to predict as this is the output layer.\n",
    "\n",
    "#### Softmax Activation \n",
    "The last layer is the Softmax Activation layer. Softmax activation enables us to calculate the output based on the probabilities. Each class is assigned a probability and the class with the maximum probability is the modelâ€™s output for the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 594,922\n",
      "Trainable params: 594,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "To reduce over-fitting, we use another technique known as Data Augmentation. Data augmentation rotates, shears, zooms, etc the image so that the model learns to generalize and not remember specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Size\n",
    "The batch size defines the number of samples that will be propagated through the network.\n",
    "\n",
    "For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.flow(X_train, Y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(X_test, Y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch\n",
    "A number of epochs means how many times you go through your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "937/937 [==============================] - 14s 15ms/step - loss: 0.0954 - acc: 0.9712 - val_loss: 0.0263 - val_acc: 0.9910\n",
      "Epoch 2/5\n",
      "937/937 [==============================] - 14s 15ms/step - loss: 0.0511 - acc: 0.9842 - val_loss: 0.0359 - val_acc: 0.9895\n",
      "Epoch 3/5\n",
      "937/937 [==============================] - 14s 15ms/step - loss: 0.0421 - acc: 0.9865 - val_loss: 0.0183 - val_acc: 0.9938\n",
      "Epoch 4/5\n",
      "937/937 [==============================] - 14s 15ms/step - loss: 0.0362 - acc: 0.9885 - val_loss: 0.0196 - val_acc: 0.9944\n",
      "Epoch 5/5\n",
      "937/937 [==============================] - 14s 15ms/step - loss: 0.0334 - acc: 0.9901 - val_loss: 0.0195 - val_acc: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27f81799a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=5, \n",
    "                    validation_data=test_generator, validation_steps=10000//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 97us/step\n",
      "\n",
      "Test accuracy:  0.9949\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print()\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achived 99.55% accuracy using this simple model. To improve the result, we can do ensembling of models. We can also use pseudo labelling to improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
